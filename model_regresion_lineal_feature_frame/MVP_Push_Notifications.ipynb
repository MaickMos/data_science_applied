{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the folder with .parquet\n",
    "folder_path = Path(\"D:/Users/maick/Desktop/Codigos/zrive-ds/data/box_builder_dataset/feature_frame.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_frame = pd.read_csv(folder_path)\n",
    "feature_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columns\n",
    "* variant id............................: -> Not include, inside of the model will be the caracteristics \n",
    "* product_type..........................: -> One Hot encoding\n",
    "* order_id..............................:\n",
    "* user_id...............................:\n",
    "* created_at..........................: are the same row, have the hour, it's innecesary -> *Delete*\n",
    "* order_date..........................: There isn't any related to the time, I think it's innecesary here -> *Delete*\n",
    "* user_order_seq.....................:\n",
    "* outcome............................:\n",
    "* ordered_before.....................:\n",
    "* abandoned_before...................:\n",
    "* active_snoozed.....................:\n",
    "* set_as_regular.....................:\n",
    "* normalised_price......................:\n",
    "* discount_pct..........................:\n",
    "* vendor................................:\n",
    "* global_popularity..................:\n",
    "* count_adults..........................:|\n",
    "* count_children........................:|\n",
    "* count_babies..........................:|-Datos imputados\n",
    "* count_pets............................:|\n",
    "* people_ex_baby........................:|\n",
    "* days_since_purchase_variant_id.....:\n",
    "* avg_days_to_buy_variant_id.........:\n",
    "* std_days_to_buy_variant_id.......:\n",
    "* days_since_purchase_product_type.:\n",
    "* avg_days_to_buy_product_type.....:\n",
    "* std_days_to_buy_product_type.....:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for >5 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_order = feature_frame.groupby(\"order_id\").outcome.sum()\n",
    "size_of_order = size_of_order[size_of_order>=5]\n",
    "feature_frame = feature_frame[feature_frame[\"order_id\"].isin(size_of_order.index)]\n",
    "feature_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_frame[\"created_at\"] = pd.to_datetime(feature_frame[\"created_at\"])\n",
    "feature_frame[\"created_at\"] = pd.to_datetime(feature_frame[\"created_at\"].dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(feature_frame[\"outcome\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the half of zeros:\n",
    "feature_frame = feature_frame.drop(feature_frame[\"outcome\"][feature_frame[\"outcome\"]== 0].sample(frac=0.5, random_state=42).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Product type: need to be categorical encoding\n",
    "feature_frame = pd.concat([feature_frame, pd.get_dummies(feature_frame[\"product_type\"])],axis=1)\n",
    "feature_frame = feature_frame.drop(\"product_type\",axis=1)\n",
    "feature_frame = feature_frame.drop(\"index\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### order_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count encoding\n",
    "feature_frame['order_id'] = feature_frame.order_id.map(feature_frame.order_id.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical encoding\n",
    "feature_frame = pd.concat([feature_frame, pd.get_dummies(feature_frame[\"order_id\"])],axis=1)\n",
    "feature_frame = feature_frame.drop(\"order_id\",axis=1)\n",
    "feature_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical encoding\n",
    "feature_frame = pd.concat([feature_frame, pd.get_dummies(feature_frame[\"vendor\"])],axis=1)\n",
    "feature_frame = feature_frame.drop(\"vendor\",axis=1)\n",
    "#feature_frame = feature_frame.drop(\"index\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count encoding\n",
    "counts = feature_frame.vendor.value_counts()\n",
    "feature_frame['vendor'] = feature_frame.vendor.map(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete: \n",
    "* user_id\n",
    "* variant_id\n",
    "* order_date\n",
    "* created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_frame = feature_frame.drop(\"variant_id\",axis=1)\n",
    "feature_frame = feature_frame.drop(\"user_id\",axis=1)\n",
    "feature_frame = feature_frame.drop(\"order_date\",axis=1)\n",
    "feature_frame = feature_frame.drop(\"created_at\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_frame = feature_frame*1\n",
    "feature_frame.columns = feature_frame.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "feature_frame_normalized = scaler.fit_transform(feature_frame)\n",
    "feature_frame_normalized = pd.DataFrame(feature_frame_normalized, columns=feature_frame.columns)\n",
    "feature_frame_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70 - 20 - 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = feature_frame_normalized[\"outcome\"]\n",
    "x = feature_frame_normalized.drop(columns=[\"outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, stratify=y, random_state=42)\n",
    "#\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, stratify=y_train, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(X_train)} \")\n",
    "print(f\"Valid: {len(X_valid)} \")\n",
    "print(f\"Test: {len(X_test)} \") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "    class_weight=\"balanced\", \n",
    "    random_state=42, \n",
    "    max_iter=500\n",
    "    )\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error log-loss logaritmic loss o binary cross entropy\n",
    "\n",
    "y_train_probs = model.predict_proba(X_train)[:, 1]  # Probabilidad de la clase 1\n",
    "train_loss = log_loss(y_train, y_train_probs)\n",
    "print(\"Log Loss en entrenamiento:\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_probs = model.predict_proba(X_test)[:, 1]\n",
    "test_loss = log_loss(y_test, y_test_probs)\n",
    "print(\"Log Loss en validación:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener probabilidades de predicción\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  # Probabilidad de la clase 1\n",
    "\n",
    "# Calcular la curva ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Graficar la curva ROC\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Línea diagonal (azar)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular precisión y recall para diferentes umbrales\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(recall, precision, marker='.', color='green', label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calcular precisión y recall para diferentes umbrales\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(recall, precision, marker='.', color='green', label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo con ajuste de pesos\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,   # Número de árboles\n",
    "    max_depth=10,       # Profundidad máxima (ajústalo si es necesario)\n",
    "    class_weight=\"balanced\",  # Manejo del desbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Usa todos los núcleos disponibles\n",
    "    )\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = model.predict(X_valid)\n",
    "print(\"Validación:\\n\", classification_report(y_valid, y_valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "print(\"Validación:\\n\", classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, \"models/model_MVP_RandomForestClassifier_01.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zrive-ds-yh3a_Qli-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
